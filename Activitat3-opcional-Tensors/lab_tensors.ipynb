{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlai_2019_lab1_tensors_todo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-tjDhXJeixW",
        "colab_type": "text"
      },
      "source": [
        "# A World of Tensors and Differentiable Computing\n",
        "\n",
        "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) ([UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) 2019)\n",
        "\n",
        "In these lab exercises we are going to see: \n",
        "\n",
        "1. What are tensors, concretely in the PyTorch framework.\n",
        "2. How to operate with them, and typical operations for deep learning modeling.\n",
        "3. How to load typical multimedia types into PyTorch tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwQSQBAt5WL2",
        "colab_type": "text"
      },
      "source": [
        "## What is a Tensor?\n",
        "\n",
        "\n",
        "A Tensor is the generalization of a vector into k dimensions.\n",
        "\n",
        "![](https://miro.medium.com/max/644/1*SGqhI_WpSaEr17wo8ycUhg.png)\n",
        "Table taken from [1].\n",
        "\n",
        "Because of this, a tensor is any k-dimensional structure, including matrices, vectors and scalars. PyTorch is a deep learning framework (https://pytorch.org) widely used for both research and production. As in any other deep learning framework, its core data structure is the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuBXBxeINqt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We first import PyTorch and Numpy libraries as fundamental tools to work with arrays and tensors\n",
        "import torch\n",
        "import numpy\n",
        "# initialize a random seed such that every execution will raise same random sequences of results\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQQFFjMQPT1n",
        "colab_type": "text"
      },
      "source": [
        "#### Creating tensors with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ChGcvuNa7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can initialize an empty structure with certain dimensions:\n",
        "a = torch.empty(5, 7)\n",
        "# and we can check its dimensionality with the .shape attribute or .size() function\n",
        "print(a.shape)\n",
        "print(a.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl28ffg_67z8",
        "colab_type": "text"
      },
      "source": [
        "Dimensions in PyTorch tensors are indexed from 0 onwards, so the first axis of size 5 is the *dim=0*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqdr870XN5Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YAYY we have created a tensor of size 5x7, but what does it contain?\n",
        "print(a)\n",
        "# Rubbish, nonsense, random stuff, it could be zero, it could be nan it could be whatever (run the )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUVjfsrOdbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can easily fill the tensor with some fixed value with the .fill_(val) function\n",
        "a.fill_(10)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U47UmToyPLHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Of course we could go k-dimensional, we just have to put more numbers in the init function\n",
        "a = torch.empty(2,4,6,8)\n",
        "print(a.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F0pCzMXkEU6",
        "colab_type": "text"
      },
      "source": [
        "#### There are functions in PyTorch to initialize some special tensors:\n",
        "\n",
        "* **torch.randn** samples from a Gaussian distribution (mean=0, std=1)\n",
        "* **torch.rand** samples from a uniform distribution [0, 1)\n",
        "* **torch.ones** creates a tensor with 1s\n",
        "* **torch.zeros** creates a tensor with 0s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC_o44hLkaqy",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Create a tensor *z* drawn from a Gaussian distribution of dimensions (16, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEidVDMrPmyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Gaussian tensor 16x1024\n",
        "z = # ...  # \n",
        "\n",
        "if z.mean().round().int().item() == 0 and z.std().round().int().item() == 1 \\\n",
        "  and z.shape[0] == 16 and z.shape[1] == 1024:\n",
        "  print('Well done! The elements in the tensor are mean centered with unit variance, and it is 16x1024!')\n",
        "else:\n",
        "  print('Wrong :( Did you use the proper torch function? Did you supply the correct tensor shape?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdGiSnOKmSyv",
        "colab_type": "text"
      },
      "source": [
        "#### Tensors data type\n",
        "\n",
        "Importantly, tensors have a data type (like numeric variables are int, float, double, etc.). We can check the type\n",
        "with the **tensor.dtype** attribute. We can also change the dtype of our tensor with a very simple cast following the data type name in the form of a function: **tensor.float()**, **tensor.int()**, **tensor.long()**, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nChnxCB0Val6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.ones(5)\n",
        "print(a.dtype)\n",
        "\n",
        "# change to float64 (aka. double)\n",
        "print(a.double().dtype)\n",
        "\n",
        "# change to float16 (aka. half)\n",
        "print(a.half().dtype)\n",
        "\n",
        "# change to int16 (aka. short)\n",
        "print(a.short().dtype)\n",
        "\n",
        "# change to int64 (aka. long)\n",
        "print(a.long().dtype)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQqUN7JGnhYl",
        "colab_type": "text"
      },
      "source": [
        "And the way to create a tensor with a specific data type at initialization is either by specifying the **dtype=torch.<dtype\\>** during the tensor initialization, or using an explicit tensor constructor like **torch.FloatTensor()**, **torch.LongTensor()**, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qur87Nn4npB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a tensor with type short()\n",
        "a = torch.empty(5, 7, dtype=torch.short)\n",
        "print(a.dtype)\n",
        "\n",
        "# Directly create a short tensor\n",
        "a = torch.ShortTensor(5, 7)\n",
        "print(a.dtype)\n",
        "\n",
        "# Remember: there should be rubbish in these results, we just explicited a data type, not any value yet! (hence random memory is depicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tw0oG0HsXOf",
        "colab_type": "text"
      },
      "source": [
        "#### Tips about tensor data types applied to deep learning:\n",
        "\n",
        "Keep in mind the following relations, they might be very useful for your future selves!\n",
        "\n",
        "* Float32 --> Data type for the neural network parameters and GPU operations!\n",
        "* Long (Int64) --> Data type for embeddings (word embeddings, char embeddings, etc.)\n",
        "* Float16 (Half) --> Data type for currently fastest GPU operations (with less precision) on advanced GPU implementations.\n",
        "\n",
        "Remember: only a sticknote for your future selves, in case you have to deal with any of the above mentioned things (embeddings, fastest GPU stuff, etc.).\n",
        "\n",
        "**TO SEE THE FULL SET OF PYTORCH TENSOR DATA TYPES, CHECK THE DOCUMENTATION AT https://pytorch.org/docs/stable/tensors.html**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK7tJePCnIoC",
        "colab_type": "text"
      },
      "source": [
        "#### Bringing the tensors from Python and Numpy\n",
        "\n",
        "You may be familiarized with Numpy and Python lists. The former one is a MUST to do any scientific programming in Python, so if you need a refresh it is recommended to have a quick review: https://becominghuman.ai/an-essential-guide-to-numpy-for-machine-learning-in-python-5615e1758301 . The latter, lists, are the inherent mechanism of Python to create a sorted structure of elements (like a k-dimensional array, as we can embed lists in lists and so on).\n",
        "\n",
        "PyTorch is very well integrated with Numpy (actually PyTorch is supposed to be an enhanced Numpy, with algebraic operations also running on GPU!) and Python. We can hence convert our Numpy and lists into PyTorch tensors VERY EASILY!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTw-m1SDVFvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a 1-D tensor from the Numpy array [1, 2, 3]\n",
        "a = torch.tensor(numpy.array([1, 2, 3]))\n",
        "\n",
        "# Creating a 1-D tensor from the Python list [1, 2, 3]\n",
        "a = torch.tensor([1, 2, 3])\n",
        "# Values 1, 2, 3\n",
        "print('Tensor a values: ', a)\n",
        "# 1 dimension of size 3\n",
        "print('Tensor a shape: ', a.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkhD-ViJqASv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-dimensional arrays are also turned into PyTorch tensors as easily as that\n",
        "A = torch.tensor(numpy.ones((16, 1024)))\n",
        "print(A.dtype)\n",
        "print(A.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J5CGBU3V5gF",
        "colab_type": "text"
      },
      "source": [
        "#### Converting tensors back to Numpy!\n",
        " \n",
        "Converting back to numpy arrays is as easy as getting the *.data* attribute of the tensor and calling its *.numpy()* casting function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzQN_EfsWLRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.rand(10, 10)\n",
        "Anpy = A.data.numpy()\n",
        "print('A type: ', type(A)) # torch.Tensor\n",
        "print('Anpy type: ', type(Anpy)) # numpy.ndarray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paUskvqytogM",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Create an **int16** *it* tensor in PyTorch (however you want) from the following numpy array *na*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdKBay3Bt3kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na = 10 * numpy.random.rand(8, 8)\n",
        "\n",
        "#TODO: create the short tensor out of 'na'\n",
        "it = # ...         #\n",
        "\n",
        "# Assertion to ensure it is the desired data type\n",
        "assert it.dtype == torch.int16, 'this tensor is not of dtype short? =('"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHyxb3EkxHaT",
        "colab_type": "text"
      },
      "source": [
        "## Operations with tensors\n",
        "\n",
        "The documentation of PyTorch tensors can be found online in: https://pytorch.org/docs/stable/tensors.html \n",
        "\n",
        "This section introduces two important types of operations in PyTorch:\n",
        "\n",
        "1. In-place operations\n",
        "2. Algebraic operations on tensors: transposing, squeezing/unsqueezing, slicing, chunking and concatenating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybnrLcAlNeJV",
        "colab_type": "text"
      },
      "source": [
        "#### In-place operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyzfwGp85o7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In-place operations are those whose function name contain an underscore '_' as in fill_(val), add_(val), etc. \n",
        "a = torch.empty(2, 2)\n",
        "a.fill_(1)\n",
        "print(a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2pf3ddI8Dfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are operations where both inplace and normal methods can be applied\n",
        "# For example to sum some value to the tensor\n",
        "a.add_(1)\n",
        "print(a) # prints a tensor of values \"2\"\n",
        "# This, though, takes no effect\n",
        "a.add(1)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtYnDTVx8UW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# So yes, you guessed right! We have to actually assign the result to an output tensor to actually \n",
        "# get the outcome of this operation\n",
        "b = a.add(1)\n",
        "print(b) # NOW it prints a tensor of values \"3\"!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwDuaN_w8ZQ3",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Do you notice an important difference between these in-place vs normal operations? Perhaps not yet... what if I tell you that I want to apply an operation upon a FloatTensor 10000x10000? Knowing that we have 32 bits per float value, compute the required memory to store that tensor in Megabytes (1 MB = 1.000.000 Bytes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyVg5u8i9NKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "total_mem = # ...         # \n",
        "print(total_mem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGXDnygK-adC",
        "colab_type": "text"
      },
      "source": [
        "### Now we can be more applied to deep learning. But with tensors, raw operations. \n",
        "\n",
        "A Neuron is defined as a linear operation of weighted sums followed by a non-linearity. We thus have a tensor of weights *w*, a scalar with the bias *b*, and a non-linearity (like ReLU *max(0, x)* that just allows the positive components to go forth in the *y* values).\n",
        "\n",
        "![](https://www.researchgate.net/profile/Haroldo_Campos_Velho2/publication/235901708/figure/fig1/AS:669443441049602@1536619162135/Artificial-neuron-Equation-neuron-output.ppm)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g1NZ0ee-rx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will have 1 input vector with 100.000 dimensions (features)\n",
        "x = torch.ones(1, 100_000)\n",
        "\n",
        "# Our weight tensor is hence, for a neuron, 100.000 x 1\n",
        "w = 0.02 * torch.randn(100_000, 1)\n",
        "\n",
        "\n",
        "# Let's define the function that will perform the operation of a neuron\n",
        "def forward_neuron(x, w, b):\n",
        "  v = x.mm(w) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n",
        "  y = v.clamp(min=0) # relu is defined as a truncation of the negative activations to zero (clamp function does the trick)\n",
        "  return y\n",
        "\n",
        "# Now we can see examples of operation through the relu\n",
        "\n",
        "# Our bias is just a scalar\n",
        "bp = 10 * torch.ones(1)\n",
        "\n",
        "print(forward_neuron(x, w, bp))\n",
        "\n",
        "# Shifting the bias quite negatively should raise zero\n",
        "bn = -10 * torch.ones(1)\n",
        "print(forward_neuron(x, w, bn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWXKiv2-xTWF",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Continuing with previous in-place vs normal operations rationale, please change the *forward_neuron* function to apply the ReLU in-place. This is very useful to save memory when constructing very deep nets. \n",
        "\n",
        "\n",
        "\n",
        "**NOTE:** for the record, this will solve the doubt you will have some day \"what is this inplace=True in the *nn.ReLU(inplace=True)* object?\" when you build neural networks with the PyTorch *torch.nn* API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecEgPQqSy6NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_neuron(x, w, b):\n",
        "  v = x.mm(w) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n",
        "  return # TODO: make the inplace clamping here\n",
        "\n",
        " print(forward_neuron(x, w, bp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y85-XopvNafw",
        "colab_type": "text"
      },
      "source": [
        "#### Transpositions and beyond\n",
        "\n",
        "Bear in mind the following FUNDAMENTAL operations to work with deep learning:\n",
        "\n",
        "* Tensor transposition: swapping dimensions in the tensor\n",
        "* Tensor chunking: breaking down a tensor into sub-pieces through a certain dimension\n",
        "* Tensor concatenation: merging different tensors into a single one.\n",
        "* Tensor squeezing/unsqueezing for dimension adjustments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJtt5zRqOCUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transposition\n",
        "A = torch.empty(10, 20, 5)\n",
        "\n",
        "# Swap axis 2 and 1\n",
        "A_21 = A.transpose(2, 1)\n",
        "\n",
        "print('{} transposed axis (2, 1) to: {}'.format(A.shape, A_21.shape))\n",
        "\n",
        "# Swap axis 2 and 0\n",
        "A_20 = A.transpose(2, 0)\n",
        "\n",
        "print('{} transposed axis (2, 0) to: {}'.format(A.shape, A_20.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiCQwQyTRBiu",
        "colab_type": "text"
      },
      "source": [
        "**NOTE:** The utlity of transpositions will be seen further when dealing with different neural architecture designs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYpl6AbfQr1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Different axis can be merged with the .view() operator\n",
        "B = A.view(200, 5)\n",
        "print('{} axis (0, 1) merged to: {}'.format(A.shape, B.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUnpBwK2RXs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that giving a wrong size in the dimensions for .view() raises an error\n",
        "try:\n",
        "  B = A.view(201, 5)\n",
        "except RuntimeError:\n",
        "  print('Wrong dimension sizes specified in .view()!')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocWwemIVR-B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chunking the tensor with .chunk() requires to specify how many chunks we want in which dimension\n",
        "# For example for tensor A: (10, 20, 5), we can chunk it into 5 sub-tensors of shape (10, 4, 5) each\n",
        "Achunks = torch.chunk(A, 5, dim=1)\n",
        "for i, achunk in enumerate(Achunks):\n",
        "  print('Chunk {} shape: {}'.format(i, achunk.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G5LySi3SeCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And tensors can be merged back to a tensor Amerged with .cat() operator, specifying in which dimension do we concatenate\n",
        "# So to go back to the same tensor as we had prior to chunking, we stack on dimension 1\n",
        "\n",
        "Amerged = torch.cat(Achunks, dim=1)\n",
        "print('Amerged shape: ', Amerged.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_0hLUvBSwDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, we may want to add additional dimensions or remove them from our tensor\n",
        "# We achieve so with .squeeze() or .unsqueeze()\n",
        "\n",
        "A = torch.empty(2, 2)\n",
        "\n",
        "# 1) Add an extra dimension in axis 0 (unsqueeze)\n",
        "\n",
        "A = A.unsqueeze(0)\n",
        "\n",
        "# 2) Add an extra dimension in axis 2\n",
        "\n",
        "A = A.unsqueeze(2)\n",
        "\n",
        "# 3) Add an extra dimension in axis 2 again\n",
        "\n",
        "A = A.unsqueeze(2)\n",
        "\n",
        "print('Current A shape after unsqueezing dimensions=(0, 2, 2): ', A.shape)\n",
        "\n",
        "# 4) Remove the dimension 0 from step (1)\n",
        "\n",
        "A = A.squeeze(0)\n",
        "\n",
        "print('Current A shape after squeezing dim=0: ', A.shape)\n",
        "\n",
        "# 5) We can remove all remaining dimensions of size 1 when we do not specify the dimension\n",
        "\n",
        "A = A.squeeze()\n",
        "\n",
        "print('Current A after squeezing all remaining dimensions of size 1: ', A.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya3h7RTxT3Nm",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "Unsqueezing and squeezing dimensions can also be achieved with the *.view()* function. \"View\" the tensor *A* to achieve the same shape as the one after step (3) in the previous cell with a single function call to *.view()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dinmam_xUI15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.empty(2, 2)\n",
        "\n",
        "# TODO: use view to unsqueeze dimensions (0, 2, 2)\n",
        "A = # ...             # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Zvpx366WlU",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6 (Grand Finale)\n",
        "\n",
        "Given the tensor *A*, shuffle the elements of the first dimension with the *random.shuffle* Python function. \n",
        "\n",
        "**Clue:** use the functions *torch.chunk*, *random.shuffle* (which acts in-place over Python lists), and *torch.cat*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n66jIY19U6JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "A = torch.rand(4, 2, 4)\n",
        "\n",
        "print('A before shuffling:\\n ', A)\n",
        "# TODO: chunk the tensor, and convert the resulting tuple into a Python list\n",
        "A = # ...         #\n",
        "\n",
        "# TODO: operate with shuffle over the list\n",
        "# ...         #\n",
        "\n",
        "# TODO: concatenate the sub-tensors in list \"A\" back to tensor \"A\"\n",
        "A = # ...          #\n",
        "\n",
        "print('A after shuffling:\\n ', A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWqua-bDWatW",
        "colab_type": "text"
      },
      "source": [
        "## Final Words \n",
        "\n",
        "Congrats! You reached the end of this introductory tutorial to PyTorch most fundamental data structure. Managing dimensions, casting dtypes, in-place operations and more are the EVERYDAY to-dos of a deep learner. So get ready to master these before delving into the coolest projects ever in which you'll build deep nets. Being confident with tensor operations is very important to properly design neural networks and avoid bugs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhDWyIL97LZ9",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] https://medium.com/datadriveninvestor/from-scalar-to-tensor-fundamental-mathematics-for-machine-learning-with-intuitive-examples-part-163727dfea8d\n",
        "\n",
        "[2] https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html\n",
        "\n",
        "[3] https://pytorch.org/docs/stable/tensors.html\n"
      ]
    }
  ]
}
