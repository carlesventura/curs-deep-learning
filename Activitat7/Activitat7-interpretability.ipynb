{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlai_2019_lab07_interpretability_todo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DFwCtc8POCs4",
        "colab_type": "text"
      },
      "source": [
        "# Interpretability of a Convolutional Neural Network\n",
        "\n",
        "**Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2019).**\n",
        "\n",
        "**Based on previous versions by [Amaia Salvador](https://www.linkedin.com/in/amaiasalvador/) ([Persontyle](https://github.com/telecombcn-dl/2017-persontyle) 2017) and [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) ([Barcelona Technology School](https://barcelonatechnologyschool.com/) 2019)**\n",
        "\n",
        "**Related [slides](https://github.com/telecombcn-dl/dlai-2019/raw/master/slides/dlai_2019_d07l1_interpretability.pdf) by [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) from [Deep Learning for Artificial Intelligence](https://telecombcn-dl.github.io/dlai-2019/) (UPC TelecomBCN 2019)**\n",
        "\n",
        "**Related video lectures from [Amaia Salvador](https://www.youtube.com/watch?v=YQvTxkPV8LQ&feature=emb_logo) and [Eva Mohedano](https://www.youtube.com/watch?v=SsHohytl1NA&feature=emb_logo) from Deep Learning for Computer Vision ([UPC TelecomBCN 2016](http://imatge-upc.github.io/telecombcn-2016-dlcv/)) and ([UPC TelecomBCN 2018](https://telecombcn-dl.github.io/2018-dlcv/)), respectively.**\n",
        "\n",
        "![Daniel Fojo](https://telecombcn-dl.github.io/2018-dlcv/img/assistants/DanielFojo-160x160.jpg)\n",
        "![Amaia Salvador](https://telecombcn-dl.github.io/2017-dlcv/img/instructors/AmaiaSalvador.jpg)\n",
        "![Eva Mohedano](https://telecombcn-dl.github.io/2018-dlcv/img/instructors/EvaMohedano.jpg)\n",
        "![Xavier Giro-i-Nieto](https://telecombcn-dl.github.io/dlai-2019/img/instructors/XavierGiro.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GfTmzaYWOCs6",
        "colab_type": "text"
      },
      "source": [
        "So far we have learned how to train models for image classification, and we have evaluated their performance in terms of accuracy. But, what did these models learn? Let's try to understand that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "pBsfbtZROCs7",
        "colab_type": "text"
      },
      "source": [
        "## Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "z0VqRLD1OCs8",
        "colab_type": "text"
      },
      "source": [
        "We will first train a simple model on CIFAR10, and then we will try to visualize how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wbuZ-2LNOCs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt  \n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception(\"You should enable GPU in the runtime menu.\")\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QW_TlN3gwai",
        "colab_type": "text"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA93AiXdJvYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_W6lJSmhFTF",
        "colab_type": "text"
      },
      "source": [
        "Declare the model.\n",
        "Related docs: [torch.nn.Conv2D](https://pytorch.org/docs/stable/nn.html#conv2d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa8cqdFGHbb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Conv2d(32, 64, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(8 * 8 * 64, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128, 10),\n",
        ")\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol1DdUJ2xMiJ",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1:**\n",
        "How many convolutional filters are there in the first layer ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auKes7t-hIrX",
        "colab_type": "text"
      },
      "source": [
        "Declare loss function and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JZPZU7mJ9Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XNzxMAehLjP",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4L0o1UXVOCtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = time.time()\n",
        "model.train()\n",
        "epochs = 5\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs to gpu; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        if i % 300 == 299:    # print every 300 mini-batches\n",
        "            print(f\"Epoch {epoch+1}/{epochs} [{i+1}/{len(trainloader)}] loss: {loss.item():.2f}\")\n",
        "\n",
        "print('Finished Training')\n",
        "print(f\"Time: {(time.time() - t):.1f}s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf07x9cIhPCi",
        "colab_type": "text"
      },
      "source": [
        "Disable gradient computation for the rest of the session, we will not need it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZimYa4cQ_eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Disable gradients computation\n",
        "torch.set_grad_enabled(False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOtCqY0ciEk2",
        "colab_type": "text"
      },
      "source": [
        "Evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0SF0_-eNF2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for data in testloader:\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {correct / total*100:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgDgCIbYyG1q",
        "colab_type": "text"
      },
      "source": [
        "The following function `display_filters` allows visualizing the convolutional filters in a layer. If the filter has more than 3 channels, it will only consider these ones in the visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MyDNM0c3OCtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_filters(weights):\n",
        "    N = int(np.ceil(np.sqrt(weights.shape[0])))\n",
        "    f, axarr = plt.subplots(N, N)\n",
        "    scaled = (weights-weights.min())/(weights.max()-weights.min()) # Scale the weights for better plotting\n",
        "\n",
        "    p = 0\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            # empty plot white when out of kernels to display\n",
        "            if p >= scaled.shape[0]:\n",
        "                krnl = torch.ones((scaled.shape[2], scaled.shape[3], 3))\n",
        "            else:\n",
        "                if scaled.shape[1] == 1: \n",
        "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
        "                    axarr[i, j].imshow(krnl)\n",
        "                # NEW: If we have 3 channels plot with color\n",
        "                elif scaled.shape[1] == 3: \n",
        "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
        "                    axarr[i, j].imshow(krnl)\n",
        "                else: # If we don't have either 1 or 3 channels, just plot the first channel\n",
        "                    krnl = scaled[p,0,:,:]\n",
        "                    axarr[i, j].imshow(krnl, cmap='gray')\n",
        "            axarr[i, j].axis('off')\n",
        "            p += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6LUODdm8_u",
        "colab_type": "text"
      },
      "source": [
        "Let's exploit the function `display_filters` to understand better the convolutional neural network that we have trained:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-37Iq0cnCsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the architecture of the model\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBRan8SqnIs5",
        "colab_type": "text"
      },
      "source": [
        "We can get the values of the weights for the first the kernels of the first layer and visualize them, since they have 3 channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "L04JpniROCtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the weights from the first layer to cpu\n",
        "weights = model[0].weight.cpu().detach() # Detach, since we do not need the gradients\n",
        "weights.shape\n",
        "display_filters(weights)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "nOiig_90OCtR",
        "colab_type": "text"
      },
      "source": [
        "Do these visualizations help you understand what the model learned? If not, later you can try to visualize the activations when these filters are convolved with the image. For now, let's move on to other visualization methods, but you can come back to this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZU8oG0H0OCtS",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 2:** Try to visualize filters in the second convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "OJWu51RtOCtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "weights=TODO\n",
        "display_filters(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8C4_OW-z4gW",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 3:** Why are the visualizations of the convolutional filters of the first layer colored, but the ones from the second layer plotted in gray-scale ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "eRuspKvcOCtV",
        "colab_type": "text"
      },
      "source": [
        "## Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EQ1BALOLOCtW",
        "colab_type": "text"
      },
      "source": [
        "We can also visualize the model's activations to our data samples to understand what the model learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Wpg4m27fOCta",
        "colab_type": "text"
      },
      "source": [
        "We will create a model that we will use as feature extractor. For that, we need to pick the layer that we want to use.\n",
        "\n",
        "**Exercise 4:** Since our NN model is of the type `nn.Sequential`, we can easily define a new one with any number of sequential layers just using a *slice*. Take a look at the layer numbers, and declare a new model names `extractor` that should have all the layers of `model` until and including the first `Linear` layer. Remember that Python slices do not include the last number (e.g. 0:5 = [0, 1, 2, 3, 4])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2EVKw9fDOCta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "extractor=model[TODO]\n",
        "\n",
        "# Visualize the resulting layer after slicing\n",
        "extractor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5SpRQEAROCtc",
        "colab_type": "text"
      },
      "source": [
        "Once we have our new neural network `extractor`, we can load the data and forward it through the network to get the activations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "F0E2bBnkOCte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feats = []\n",
        "for data in testloader:\n",
        "    inputs, _ = data[0].to(device), data[1].to(device)\n",
        "    feats.append(extractor(inputs).cpu().numpy())\n",
        "\n",
        "feats = np.concatenate(feats)\n",
        "\n",
        "# Check that the extracted features correspond to the 10000 test images and the\n",
        "# amount of neurons in the last considered fully connected layer\n",
        "feats.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IrtfrserOCtf",
        "colab_type": "text"
      },
      "source": [
        "Once we have extracted activations for all samples in our test set, we will use different visualization tools to understand what the model learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fzuY701lOCtg",
        "colab_type": "text"
      },
      "source": [
        "### Finding per unit top K samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fdl0LUp9OCtg",
        "colab_type": "text"
      },
      "source": [
        "Let's now find the K images with highest activation for each neuron in the layer, using the original extracted activations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_QeffN_lOCth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testimages = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
        "\n",
        "K = 10\n",
        "idxs_top10 = np.argsort(feats, axis=0)[::-1][0:K, :]\n",
        "picked_samples = np.zeros((K, 128, 32, 32, 3), dtype=float)\n",
        "for i in range(idxs_top10.shape[0]):\n",
        "    for j in range(idxs_top10.shape[1]):\n",
        "        picked_samples[i, j, :, :, :] = np.asarray(testimages[idxs_top10[i, j]][0])/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKk2rrKn5kdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "picked_samples.shape\n",
        "# The shape of the tensor corresponds to: \n",
        "# (n_images,n_units,nb_rows,nb_cols,nb_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "S2CQfk0xOCtj",
        "colab_type": "text"
      },
      "source": [
        "```picked_samples``` now contains the 10 images with highest activation for each neuron. Let's visualize these images for some neurons:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XcD-dvkEOCtj",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 5:** The following array defines which units are selected to display their top K images. However, the visualization will crash because one of the values is wrong. Find it and correct it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEv_BE5gWv1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "units = [1, 2, 4, 14, 23, 29, 128] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bZgyRjd_OCtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nunits = len(units)\n",
        "ims = picked_samples[:, units, :, :].squeeze()\n",
        "\n",
        "def vis_topk(ims,units):\n",
        "    f, axarr = plt.subplots(ims.shape[0],ims.shape[1],figsize=(10,10))\n",
        "    \n",
        "    for i in range(ims.shape[0]):\n",
        "        for j in range(ims.shape[1]):\n",
        "\n",
        "            axarr[i,j].imshow(ims[i,j,:,:,:])\n",
        "            axarr[i,j].axis('off')\n",
        "            axarr[0,j].set_title('unit '+ str(units[j]))\n",
        "            \n",
        "vis_topk(ims, units)\n",
        "ims.shape\n",
        "#(n_ims,n_units_picked,nb_rows,nb_cols,nb_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "mjLwlxRDOCtn",
        "colab_type": "text"
      },
      "source": [
        "Do all units respond to distinguishable concepts? Are there units that respond to similar things? Did you find any units with semantic meaning? You can try for different units and see what images they like the most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "HXqR-aIjOCtn",
        "colab_type": "text"
      },
      "source": [
        "### Occlusion experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "m7Rvv_KQOCtn",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will find which parts of the image contribute to the activation the most. We will create a small NxN occluder patch and slide it through each image with a stride of 2, and then feed each occluded image through the network. Later, we will compute the difference between the activations between the original image and the occluded ones, and create a difference map that we will overlay over the image for visualization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "K8bFGSjbOCto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def occ_images(ims, occ=(11, 11), stride=4):\n",
        "    \n",
        "    # Reshape to put top images for all units stacked together\n",
        "    ims = np.rollaxis(ims, 1, 0)\n",
        "    ims = np.reshape(ims, (ims.shape[0]*ims.shape[1], ims.shape[2], ims.shape[3], ims.shape[4]))\n",
        "    ims_acc = ims\n",
        "    \n",
        "    # slide \n",
        "    npos = 1\n",
        "    st = int(np.floor(occ[0]/2))\n",
        "    \n",
        "    # slide occluder, set pixels to 0 and stack to matrix\n",
        "    for i in range(st, ims.shape[1], stride):\n",
        "        for j in range(st, ims.shape[2], stride):\n",
        "            ims_occ = copy.deepcopy(ims)\n",
        "            ims_occ[:, i-st:i+occ[0]-st, j-st:j+occ[1]-st, :] = 0\n",
        "            ims_acc = np.vstack((ims_acc, ims_occ))\n",
        "            npos += 1\n",
        "            \n",
        "    return ims_acc\n",
        "\n",
        "# Feed the previously obtained top K images through the occluding function\n",
        "ims_acc = occ_images(ims)\n",
        "print(ims_acc.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "feAZWiU6OCtp",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize some of the images with the occluded region in different positions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "217dgpnvOCtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axarr = plt.subplots(10,ims.shape[1],figsize=(10,10))\n",
        "ims_acc_r = ims_acc.reshape(ims_acc.shape[0]//(ims.shape[0]*ims.shape[1]),\n",
        "                                ims.shape[1], ims.shape[0],\n",
        "                                ims_acc.shape[1], ims_acc.shape[2], ims_acc.shape[3])\n",
        "for i in range(10):\n",
        "    for j in range(ims.shape[1]):\n",
        "        axarr[i,j].imshow(ims_acc_r[2*i,j,0,:,:,:])\n",
        "        axarr[i,j].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "78O7Buq7OCtr",
        "colab_type": "text"
      },
      "source": [
        "We should pick an occluder that is large enough to cover significant parts of objects. 11x11 is the default one, but you can experiment with other sizes and see their effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Ey_G8l56OCtr",
        "colab_type": "text"
      },
      "source": [
        "```ims_acc``` contains all images with the occluder set at different positions. Let's run these through our extractor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1GoGm66eOCts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a PyTorch tensor from the occluded images\n",
        "ims_acc_tensor = torch.tensor(ims_acc, dtype=torch.float).permute(0, 3, 1, 2).to(device)\n",
        "\n",
        " # Normalize data\n",
        "ims_acc_tensor = (ims_acc_tensor-0.5)/0.5 \n",
        "\n",
        "# Feed the occluded imaged through the network\n",
        "output = extractor(ims_acc_tensor)\n",
        "\n",
        "# Transform the output tensor to numpy\n",
        "feats_occ = output.cpu().numpy()\n",
        "feats_occ.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "r4yB4yq8OCtt",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the features, we can compute the difference between the original activation and the activation for each of the occluded images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "G5LvThK_OCtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feats_r = np.reshape(feats_occ,(feats_occ.shape[0] // (ims.shape[0] * ims.shape[1]),\n",
        "                                ims.shape[1], ims.shape[0], feats_occ.shape[1]))\n",
        "\n",
        "distances = feats_r[0] - feats_r[1:] # original activation minus all the occluded ones\n",
        "distances = np.rollaxis(distances, 0, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Es1fCNVJOCtu",
        "colab_type": "text"
      },
      "source": [
        "Reshaping the distance array into a 2D map will give a mask that we can display on top of the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wFv50NiWOCtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = int(np.sqrt(distances.shape[3]))\n",
        "\n",
        "heatmaps = np.zeros((distances.shape[0],distances.shape[1],distances.shape[3]))\n",
        "for i in range(len(units)):    \n",
        "    heatmaps[i] = distances[i,:,units[i],:]\n",
        "heatmaps.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uTfU0CfoOCtw",
        "colab_type": "text"
      },
      "source": [
        "Let's declare and run a function to display the masks on top of the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9YaOuiEeOCtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vis_occ(ims,heatmaps,units,th=0.5,sig=2):\n",
        "    \n",
        "    from scipy.ndimage.interpolation import zoom\n",
        "    from scipy.ndimage.filters import gaussian_filter\n",
        "    import copy\n",
        "    \n",
        "    ims = np.rollaxis(ims,1,0)\n",
        "    \n",
        "    s = int(np.sqrt(heatmaps.shape[2]))\n",
        "    heatmaps = np.reshape(heatmaps,(heatmaps.shape[0],heatmaps.shape[1],s,s))\n",
        "    \n",
        "    f, axarr = plt.subplots(ims.shape[1],ims.shape[0],figsize=(10,10))\n",
        "    \n",
        "    for i in range(ims.shape[0]):\n",
        "        for j in range(ims.shape[1]):\n",
        "            \n",
        "            im = copy.deepcopy(ims[i,j,:,:,:])\n",
        "            mask = copy.deepcopy(heatmaps[i,j,:,:])\n",
        "            \n",
        "            # Normalize mask\n",
        "            mask = (mask - np.min(mask))/(np.max(mask)-np.min(mask))\n",
        "            # Resize to image size\n",
        "            mask = zoom(mask,float(im.shape[0])/heatmaps.shape[-1],order=1)\n",
        "            # Apply gaussian to smooth output\n",
        "            mask = gaussian_filter(mask,sigma=sig)\n",
        "            # threshold to obtain mask out of heatmap\n",
        "            mask[mask>=th] = 1\n",
        "            mask[mask<th] = 0.3\n",
        "            \n",
        "            # Mask all image channels\n",
        "            for c in range(3):\n",
        "                im[:,:,c] = im[:,:,c]*mask\n",
        "                \n",
        "            axarr[j,i].imshow(im)\n",
        "            axarr[j,i].axis('off')\n",
        "            axarr[0,i].set_title('unit '+ str(units[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2fC_XiR7m3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis_occ(ims,heatmaps,units,th=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "snbVm8uJOCty",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 6**: The obtained masks are of course not perfect, but we get to see what parts of the image are most relevant for each unit in the layer. Are these masks what you expected? Do the picked neurons maximally respond to what you have previously guessed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LHiA6x3tOCty",
        "colab_type": "text"
      },
      "source": [
        "### Additional: t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PqJSrkOsOCty",
        "colab_type": "text"
      },
      "source": [
        "Here we will display our learned features in a 2D space using t-SNE. To do this, we will use the provided function in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). We will also reduce the dimensionality with PCA before running t-SNE to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bEQdTfCqOCty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = time.time()\n",
        "# Reduce dimensionality with PCA before TSNE\n",
        "n = 20\n",
        "pca = PCA(n_components=n)\n",
        "feats_nd = pca.fit_transform(feats)\n",
        "\n",
        "# should do more iterations, but let's do the minimum due to time constraints\n",
        "n_iter = 800\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=n_iter)\n",
        "feats_2d = tsne.fit_transform(feats_nd)\n",
        "\n",
        "print(f\"Time: {(time.time() - t):.1f}s\")\n",
        "feats_2d.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3K13s3MPOCt2",
        "colab_type": "text"
      },
      "source": [
        "Once we have our 2D features, we can display them with their class labels, to see if the learned features are discriminative enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kJOvuiypOCt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "# 0: airplane\n",
        "# 1: automobile\n",
        "# 2: bird\n",
        "# 3: cat\n",
        "# 4: deer\n",
        "# 5: dog\n",
        "# 6: frog\n",
        "# 7: horse\n",
        "# 8: ship\n",
        "# 9: truck\n",
        "labels = [y for _, y in testimages]\n",
        "s = 20 # area of samples. increase if you don't see cclear colors\n",
        "plt.scatter(feats_2d[:,0], feats_2d[:,1], c=labels, cmap=plt.cm.get_cmap(\"jet\", 10), s=20) # 10 because of the number of classes\n",
        "plt.clim(-0.5, 9.5)\n",
        "cbar = plt.colorbar(ticks=range(10))\n",
        "cbar.ax.set_yticklabels(cifar_labels);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YpFxGI70OCt3",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 7:** What categories seem to be easier for our model? Which ones are confusing?"
      ]
    }
  ]
}